# Wallet Analysis System Refactoring Implementation Plan (Revised)

**Status Update (as of current conversation):** BullMQ job queue infrastructure is now COMPLETE. All A tasks (A1-A4) and B tasks (B1-B5) have been successfully implemented and integrated. The system now has a robust job processing system with Redis-based locking, deduplication, WebSocket progress tracking, and comprehensive API endpoints for job management. The queue system is ready for production use with 4 specialized queues handling wallet operations, analysis operations, similarity operations, and enrichment operations. 

## Recently Completed (Backend & Job implementation)


# ======== Immediate focus ==========

This section outlines the critical tasks for the upcoming development cycle(s), prioritized to enhance core functionality and user experience.


### High Priority (Next Working Session Focus)

1.  **Implement AI Expert Analysis Interpreter (Phase 6, Use Case 2: Similarity Analysis)**
    *   **Goal:** Build the end-to-end functionality for a user to select a group of wallets, run a similarity analysis, and receive a user-friendly, AI-interpreted report on the dashboard.
    *   **Status:** `To Do`
    *   **Architectural Plan:**
        *   **A. Backend - API Endpoint:**
            *   Create a new module for analysis tasks (e.g., `src/analyses/analyses.module.ts`).
            *   Implement a new endpoint: `POST /api/v1/analyses/similarity`.
            *   This endpoint will accept a list of wallet addresses and the analysis type (`capital` or `binary`).
            *   For now, this will be a synchronous request for simplicity. We will address long-running jobs in Phase 7.
        *   **B. Backend - Service Logic:**
            *   The new `AnalysesController` will call an `AnalysesService`.
            *   The `AnalysesService` will orchestrate the following:
                1.  Call the existing `SimilarityService` to generate the raw text-based similarity report.
                2.  Instantiate a new, modular `LLMService`.
                3.  Pass the raw report to the `LLMService` along with the specialized prompt from `docs/behavioral_reconstruction_task.md`.
            *   The `LLMService` will return the final, cleaned-up summary.
            *   The endpoint will return this summary to the frontend.
        *   **C. Frontend - UI/UX:**
            *   Design a new UI section or page (e.g., "Analysis Lab" or similar).
            *   Add functionality for a user to select multiple wallets (e.g., from their favorites list or via search).
            *   A button ("Run Similarity Analysis") will call the new backend endpoint.
            *   The UI must handle a loading state while the analysis runs.
            *   Display the returned AI-generated report using clean formatting (e.g., using Markdown rendering to preserve headings, lists, etc.).

### Medium Priority (Follow-on Tasks After Initial Deployment)

### 4.4 Service Layer Interaction (THINK LIKE MAINTANCE - CHECK DB SCHEMA FOR STATS WE HAVE LOTS) ; THINK VERY GOOD HERE!! 
- API controllers/handlers (e.g., in `src/api/controllers/`) will:
    1. Perform request validation and parse parameters.
    2. Authenticate the request via API key, retrieve `userId`.
    3. Call `databaseService.logActivity()` at the start of the request, and then again for success/failure outcomes, capturing duration, request parameters, and `userId`.
    4. Instantiate and invoke the appropriate NestJS-wrapped service(s) from locations like `src/behavior/behavior.service.ts`, `src/database/database.service.ts`, etc.
    5. **Note on User Context in Services (Refinement of original Task 4 "Refactor Services for API User Context"):** 
        - Core underlying services (e.g., `OriginalBehaviorService` in `src/wallet_analysis/core/`) will generally remain agnostic to API `userId` and direct `logActivity` calls. This maintains their reusability for CLIs, workers, etc.
        - NestJS wrapper services (e.g., `src/behavior/behavior.service.ts`) that are directly called by API controllers will receive the `userId` if they orchestrate multiple distinct, significant sub-operations that require separate activity log entries beyond the main API call logging. For simple pass-through calls from the wrapper to an original service (like the current `BehaviorService.getWalletBehavior`), controller-level logging is sufficient.
        - The original services will not be modified to directly accept `userId` for logging purposes unless a compelling case arises for a specific service where internal steps are complex and directly relevant to user-initiated API actions.
    6. Format service responses for the API.

### Lower Priority (Defer for Now)

1 **Pilot "Wallet Comparison" / "Find Similar Wallets" Feature:**
    *   **Goal:** Introduce initial functionality for comparing the current wallet to others or finding wallets with similar behavioral/performance characteristics.
    *   **Status:** `To Do`
    *   **Backend Tasks:**
        *   Verify `SimilarityService` (Phase 2.1.3) is suitable for providing basic similarity scores or vectors.
        *   Design and implement a new API endpoint, e.g., `GET /api/v1/wallets/{walletAddress}/similar?limit=5`. This endpoint should leverage `SimilarityService` to find and return a list of wallet addresses and perhaps a key similarity metric/reason.
    *   **Frontend Tasks:**
        *   Design a simple UI element (e.g., a button "Find Similar" on `WalletProfileLayout.tsx` or within a specific tab like "Overview" or "Behavioral Patterns").
        *   Upon user interaction, call the new API endpoint.
        *   Display the returned list of similar wallet addresses (e.g., in a modal or a small, dismissible section). For each similar wallet, display its address and allow navigation to its profile.
    *   **Note:** Keep this a pilot; focus on a simple, demonstrable version. The complexity will be in the `SimilarityService`'s logic, not necessarily the UI.

2 **Health Monitoring & System Reliability Setup:**
    *   **Goal:** Implement comprehensive health monitoring and system reliability checks to ensure the API's stability and performance.
    *   **Status:** `To Do`
    *   **Current Implementation:**
        *   Basic health check endpoint (`GET /api/v1/health`) implemented with:
            *   API responsiveness check
            *   Database connectivity check
            *   Response time metrics
            *   Version information
    *   **Future Health Checks to Add:**
        *   **Memory Usage:**
            *   Track heap usage, total memory
            *   Alert on high memory consumption
        *   **Disk Space:**
            *   Monitor available storage
            *   Alert on low disk space
        *   **External Service Dependencies:**
            *   Helius API health check
            *   Other third-party service status
        *   **Performance Metrics:**
            *   Request latency percentiles
            *   Error rates
            *   Database query performance
    *   **Monitoring Platform Setup:**
        *   **Initial Setup (UptimeRobot):**
            *   Create account at [UptimeRobot](https://uptimerobot.com/)
            *   Add monitor for health endpoint
            *   Configure alerts (email notifications)
            *   Set monitoring interval (5 minutes)
            *   Configure alert thresholds
        *   **Future Enhancement (Prometheus + Grafana):**
            *   Install Prometheus for metrics collection
            *   Set up Grafana dashboards for:
                *   Response time trends
                *   Error rate monitoring
                *   Resource usage graphs
                *   Database performance metrics
    *   **Implementation Steps:**
        1.  Deploy current health endpoint to production
        2.  Set up UptimeRobot monitoring
        3.  Add additional health checks incrementally
        4.  (Future) Migrate to Prometheus + Grafana for advanced metrics
    *   **Note:** This task requires server access for deployment and configuration. Defer until server access is available.


# Working plan

## Phase 1: Core Analysis Logic Extraction (Complete)

### 1.1 Reorganize Existing Types (Complete)
Location: `src/types/`
- Types are consolidated in `analysis.ts`, `behavior.ts`, `correlation.ts`, `similarity.ts`, `wallet.ts`, etc.

### 1.2 Extract Core Analysis Logic (Complete)

#### 1.2.1 Correlation Analysis
Location: `src/wallet_analysis/core/correlation/analyzer.ts`
- `CorrelationAnalyzer` class encapsulates pairwise scoring, clustering, and global token stats calculation, migrated from `activityCorrelator.ts`.

#### 1.2.2 Behavior Analysis
Location: `src/wallet_analysis/core/behavior/analyzer.ts`
- `BehaviorAnalyzer` class encapsulates sequence building, metrics calculation, and classification, migrated from `wallet-behavior-analyzer.ts`.

#### 1.2.3 Similarity Analysis
Location: `src/wallet_analysis/core/similarity/analyzer.ts`
- `SimilarityAnalyzer` class encapsulates vector creation (capital/binary) and similarity scoring (cosine), migrated from `walletSimilarity.ts`.

#### 1.2.4 KPI Reporting Analysis
Location: `src/wallet_analysis/core/reporting/kpi_analyzer.ts`
- `KPIComparisonAnalyzer` encapsulates comparative report generation logic from `kpi-comparison-report.ts`.

### 1.3 Extract Utility Functions (Complete)

#### 1.3.1 PNL Calculation
Location: `src/wallet_analysis/utils/pnl_calculator.ts`
- Contains `calculateWalletPnl` and `calculatePnlForWallets`, extracted from `activityCorrelator.ts`.

#### 1.3.2 Reporting Utilities
Location: `src/wallet_analysis/reporting/report_utils.ts`
- Contains `generateBehaviorReport`, `generateCorrelationReport`, `generateSimilarityReport`, and `saveReport`, consolidating logic from multiple scripts.

## Phase 2: Service Layer Enhancement (Largely Complete)

### 2.1 Analysis Services
Location: `src/wallet_analysis/services/`

#### 2.1.1 `CorrelationService`
- Uses `CorrelationAnalyzer`.
- Integrates database fetching (via `DatabaseService`).
- Integrates bot filtering logic (from `activityCorrelator.ts`).
- Integrates PNL calculation (using `pnl_calculator.ts`).

#### 2.1.2 `BehaviorService`
- Uses `BehaviorAnalyzer`.
- Integrates database fetching (via `DatabaseService`).

#### 2.1.3 `SimilarityService`
- Uses `SimilarityAnalyzer`.
- Integrates database fetching (via `DatabaseService`).
- Integrates shared token analysis (from `walletSimilarity.ts`).

#### 2.1.4 `ReportingService`
- Uses `BehaviorService`, `CorrelationService` (optional), `SimilarityService` (optional), `KPIComparisonAnalyzer`.
- Uses `report_utils.ts` to generate and save various report types (individual behavior, comparative behavior, correlation, similarity).

### 2.2 Enhance Existing Services (Largely Complete)

- `DatabaseService`: Refactored into a class. Imports in older scripts like `helius-analyzer.ts` need updating. References to removed `lastSignatureAnalyzed` field were cleaned up. - **COMPLETE & VERIFIED**
- `HeliusApiClient`: Assumed functional based on plan. - **VERIFIED**
- `HeliusTransactionMapper`: Assumed functional based on plan. - **VERIFIED**

## Phase 3: Script Refactoring (Complete)

**Goal:** Transform scripts in `src/scripts/` to act as simple orchestrators or entry points that primarily call the new services.

### 3.1 `activityCorrelator.ts`
- **Status: Complete**
- The script now handles CLI argument parsing (wallet source, excludeMints, timeRange).
- It instantiates `DatabaseService`, `CorrelationService`, and `ReportingService`.
- It creates `CorrelationAnalysisConfig` with CLI args.
- It calls `reportingService.generateAndSaveCorrelationReport` to orchestrate the analysis and reporting.
- Internal logic for fetching, filtering, PNL calculation, analysis, clustering, and report generation has been removed.

### 3.2 `wallet-behavior-analyzer.ts`
- **Status: Complete** 
- The script now handles CLI argument parsing (walletAddress, label, excludeMints, timeRange).
- It instantiates `DatabaseService`, `BehaviorService`, and `ReportingService`.
- It creates `BehaviorAnalysisConfig` with CLI args.
- It calls `behaviorService.analyzeWalletBehavior`.
- It calls `reportingService.generateAndSaveIndividualBehaviorReport`.
- Internal logic for fetching, analysis, and reporting has been removed.

### 3.3 `kpi-comparison-report.ts`
- **Status: Complete** 
- The script now handles CLI argument parsing (walletList file, excludeMints, timeRange).
- It instantiates `DatabaseService`, `BehaviorService`, `KPIComparisonAnalyzer`, and `ReportingService`.
- It creates `BehaviorAnalysisConfig` with CLI args.
- It calls `reportingService.generateComparativeBehaviorReport`.
- Internal logic for fetching, analysis, and reporting has been removed.

### 3.4 `walletSimilarity.ts`
- **Status: To Do**
- **Goal:** Update `main` function to orchestrate service calls.
- **Required Refactoring Steps:**
    - Parse CLI args (wallet source, vector type, potentially timeRange/excludeMints).
    - Instantiate `DatabaseService` and `SimilarityService`.
    - Create `SimilarityAnalysisConfig` (potentially including timeRange/excludeMints).
    - Instantiate `ReportingService` (injecting `SimilarityService`).
    - Call `reportingService.generateAndSaveSimilarityReport(walletAddresses, vectorType)`.
    - Remove internal logic for fetching, shared token analysis, vector creation, similarity calc, and reporting.

### 3.5 `helius-analyzer.ts`
- **Status: Complete**
- **Description:** The script has been refactored to act as an orchestrator, leveraging dedicated services (`HeliusSyncService`, `PnlAnalysisService`, `ReportingService`, `DatabaseService`) for data synchronization, analysis, and reporting. **Crucially, its logic for determining whether to re-run P/L analysis has been updated to use the new `analyzedTimestampStart`/`analyzedTimestampEnd` fields in conjunction with overall fetched timestamps from `HeliusSyncService`, fixing previous issues with `--fetchOlder` and ensuring more robust re-analysis triggers.**
- **Key Components Used:**
    - `HeliusSyncService`: Manages data fetching, mapping, and storage. Its role in setting `firstProcessedTimestamp` and `newestProcessedTimestamp` on the `Wallet` model has been verified as correct for the new analysis trigger logic.
    - `PnlAnalysisService`: Orchestrates P/L and stats calculation. **It now correctly updates `analyzedTimestampStart` and `analyzedTimestampEnd` on the `Wallet` model after a successful analysis.**
    - `ReportingService`: Generates and saves Markdown/CSV reports.
    - `DatabaseService`: Used directly by the script for managing `AnalysisRun` records.
    - `cliUtils` / `displayUtils`: For parsing and displaying.
- **Refactoring Steps Taken (Summary):**
    1. Extracted core swap logic to `SwapAnalyzer`.
    2. Extracted core stats logic to `AdvancedStatsAnalyzer`.
    3. Created `HeliusSyncService` for data fetching/syncing (verified its wallet state updates).
    4. Created `PnlAnalysisService` for orchestrating analysis (updated its wallet state updates).
    5. Updated `ReportingService` and `report_utils.ts` for PNL reports.
    6. Refactored `helius-analyzer.ts` script to use these services and manage `AnalysisRun` persistence, **including the refined analysis trigger logic.**

## Phase 4: API Layer Implementation (COMPLETE)

**Goal:** Develop a robust API layer to expose analysis functionalities for client applications (e.g., dashboards), enable user tracking, and support scalable architecture. This phase transitions the system from script-driven to service-accessible.

### 4.1 API Design Principles & Technology

#### 4.1.1 Foundational Principles
- **RESTful Architecture:** Adhere to REST principles for resource naming, HTTP methods, and status codes.
- **Statelessness:** API endpoints will be stateless where possible, not relying on server-side session memory between requests.
- **Versioning:** API endpoints will be versioned (e.g., `/api/v1/...`) to allow for future iterations without breaking existing clients.
- **Consistent Error Handling:** Implement a standardized error response format.
- **Authentication:** Secure endpoints using API key-based authentication.

#### 4.1.2 API Technology Stack & Framework Selection
- **Context:** The backend is TypeScript-based, utilizing Prisma. The API layer should integrate seamlessly.
- **Recommendation:** A Node.js framework is highly recommended.
    - **NestJS:** An opinionated, TypeScript-first framework promoting robust architectural patterns (modules, services, controllers) that align well with the existing service-oriented structure. Provides strong out-of-the-box support for OpenAPI (Swagger) generation, validation, and more.

#### 4.1.3 API Documentation (OpenAPI/Swagger)
- **Requirement:** Implement automated generation of OpenAPI (Swagger) documentation from the outset. 
- **Benefits:** Provides a live, accurate API reference crucial for dashboard developers, simplifies development, testing, and future maintenance.
- **Implementation:** 
    - If NestJS is chosen, it has excellent built-in support.
    - For Fastify, libraries like `fastify-swagger` can be integrated.

- **Auto-generating client SDKs from routes**

#### 4.1.4 Configuration Management (API & Keys)
- **API Keys:**
    - Generation: Securely generate API keys for `User` models.
    - Storage: Store API keys hashed in the database (e.g., using `bcrypt`). The actual key is shown to the user once upon creation.
- **Application Configuration:**
    - Manage API application settings (database connection strings, logging levels, JWT secrets if used later) via environment variables (e.g., using a `.env` file with `dotenv` library), consistent with Prisma's `DATABASE_URL` handling.

### 4.2 Core API Components & User Tracking

#### 4.2.1 Database Schema for User Tracking (to be added to `prisma/schema.prisma`)
Two new models will be introduced:

```prisma
// In prisma/schema.prisma

model User {
  id           String    @id @default(cuid()) // Or use autoincrement Int
  apiKey       String    @unique // For API authentication
  description  String?   // e.g., "Dashboard Primary Access", "Analyst X"
  createdAt    DateTime  @default(now())
  lastSeenAt   DateTime?
  isActive     Boolean   @default(true)
  activityLogs ActivityLog[]
  // Optional: Link to user-specific settings or saved wallet lists
}

model ActivityLog {
  id                 String    @id @default(cuid()) // Or use autoincrement Int
  userId             String
  user               User      @relation(fields: [userId], references: [id])
  actionType         String    // e.g., 'get_wallet_summary', 'get_token_performance', 'run_pnl_analysis'
  timestamp          DateTime  @default(now())
  requestParameters  Json?     // Input parameters for the action
  status             String    // 'SUCCESS', 'FAILURE', 'INITIATED'
  durationMs         Int?      // Duration of the action
  errorMessage       String?
  sourceIp           String?   // Optional: for additional context

  @@index([userId])
  @@index([actionType])
  @@index([timestamp])
}
```

#### 4.2.2 Authentication
- Clients will include an `X-API-Key` header in requests.
- A middleware in the API framework will validate the API key against the `User` table and identify the `userId`.

#### 4.2.3 `DatabaseService` Enhancements (`src/services/database-service.ts`)
The existing `DatabaseService` will be augmented with methods to:
- Create and validate users/API keys.
- Record entries in the `ActivityLog` table (start, success, failure of actions).
- Update `lastSeenAt` for users.

### 4.3 API Endpoints (Illustrative)
All endpoints will be under a base path like `/api/v1/`.

#### 4.3.1 Wallet & General Information Endpoints
-   **`GET /wallets/{walletAddress}/summary`**: (Implemented)
    *   **Purpose:** Provides a comprehensive overview for a wallet, serving as the primary snapshot. It includes data for the "Sticky Summary Bar" and "General ACC Info" in a dashboard context. This endpoint also incorporates aggregated metrics previously envisioned for a separate `/aggregated-metrics` endpoint.
    *   **Data:** Wallet address, last active timestamp, days active (derived), key account-level performance indicators (e.g., latest PNL, token win rate from `AdvancedStatsResult`), a high-level behavior classification (from `BehaviorService`), and the full raw `AdvancedStatsResult` and `BehaviorMetrics` objects for clients needing more detail from this primary call.
    *   **Services Used:** `DatabaseService` (for `Wallet`, `AdvancedStatsResult`), `BehaviorService`.
    *   **Data Derivation Notes for "Last Active" and "Days Active":**
        *   **Wallet's "Last Active Timestamp":** Determined based on `Wallet.newestProcessedTimestamp` or the run timestamp from `AdvancedStatsResult`.
        *   **Wallet's "Analyzed Range":** `Wallet.analyzedTimestampStart` to `Wallet.analyzedTimestampEnd` now accurately reflects the period covered by the last canonical PNL analysis.
        *   **Wallet's "Days Active":** Calculated based on `Wallet.firstProcessedTimestamp` and the last active timestamp.
-   **`GET /wallets/{walletAddress}/info`**: (Potentially merged into summary or separate for more static details - currently merged into `/summary`)

#### Next Endpoints to Implement (Dashboard Focus - after immediate focus tasks):

1.  **`GET /wallets/{walletAddress}/token-performance`**: (Implemented - details below for completeness)
    *   **Plan Reference:** Section 4.3.2.
    *   **Purpose:** Powers the "aggregated data on token level" view for the dashboard.
    *   **Data:** Paginated and sortable list of token performance records from the latest `AnalysisResult` for the given wallet. Includes fields like `tokenAddress`, `netAmountChange`, `netSolProfitLoss`, etc.
    *   **Query Params:** `page`, `pageSize`, `sortBy`, `sortOrder`.
    *   **Services Used:** `DatabaseService`, `TokenPerformanceService`.

2.  **`GET /wallets/{walletAddress}/pnl-overview`**: (Implemented - details below for completeness)
    *   **Purpose:** Provide a detailed Profit and Loss (PNL) overview for a wallet, distinct from the general summary.
    *   **Data Considerations:** Includes overall realized PNL, total SOL spent/received, swap-level win rates, trade volumes, and various advanced trading statistics derived by `PnlAnalysisService` (e.g., median PNL per token, profit consistency, efficiency scores). This offers an in-depth financial performance view.
    *   **Services Used:** `PnlAnalysisService`.
    *   **Note:** This endpoint provides detailed financial metrics that are complementary to the broader `/summary` endpoint.

#### 4.3.2 Token Performance View Endpoints
-   **`GET /wallets/{walletAddress}/token-performance`**: (Implemented)
    *   **Purpose:** Powers the "aggregated data on token level" view for the dashboard.
    *   **Data:** Paginated and sortable list of token performance records from the latest `AnalysisResult` for the given wallet. Includes fields like `tokenAddress`, `netAmountChange`, `netSolProfitLoss`, etc.
    *   **Query Params:** `page`, `pageSize`, `sortBy`, `sortOrder`.
    *   **Services Used:** `DatabaseService` (via `TokenPerformanceService`).

#### 4.3.3 Aggregated Metrics View Endpoints
-   **`GET /wallets/{walletAddress}/aggregated-metrics`**: (Consolidated into `/summary`)
    *   **Status:** This endpoint's intended functionality (comprehensive metrics from `AdvancedStatsResult` and other overall statistics) is now provided as part of the `GET /wallets/{walletAddress}/summary` endpoint (see section 4.3.1). No separate endpoint is planned.
    *   **Services Used:** N/A (Functionality covered by `DatabaseService` as used by `/summary`).

#### 4.3.4 Behavior View Endpoints
-   **`GET /wallets/{walletAddress}/behavior-analysis`**: (Implemented)
    *   **Purpose:** Powers the "behavior" view.
    *   **Data:** Structured output from `BehaviorService` including trader classification, pattern timelines, consistency metrics, efficiency scores, strategic tags, temporal behavior details. This will be more granular than the summary classification.
    *   **Services Used:** `BehaviorService`.

#### 4.3.5 Analysis Triggering Endpoints (Optional - for future dashboard control)
-   **Unified Trigger Pattern (Primary for Dashboard Interaction):**
    -   **`POST /analyses/wallets/{walletAddress}/trigger-analysis`**: (Future)
        *   **Purpose:** Allow the dashboard or other clients to initiate a new analysis (e.g., PNL & Stats, Behavior, or all relevant types) for a specific wallet, or to request a re-analysis of existing data. This endpoint enqueues the analysis task(s).
        *   **Request Body:** May include `analysisType: ('pnl' | 'behavior' | 'all')`, and type-specific configuration options (e.g., time range, parameters for `PnlAnalysisService` or `BehaviorService`). Could also include a flag like `forceRefetch: boolean` to control fetching of underlying transaction data.
        *   **Response:** `202 Accepted` with a `jobId` (e.g., `{ "jobId": "some-unique-identifier" }`).
        *   **Services Used:** An orchestrator service (or enhancements to `DatabaseService`) to create an `AnalysisJob` record (with status 'pending'), manage the workflow (e.g., fetching transactions if required, then running analyses), and add the job to a queue (see Phase 7 for Job Queue System).
    -   **`GET /analyses/jobs/{jobId}/status`**: (Future)
        *   **Purpose:** Allow the client to poll for the status of an analysis job initiated by `trigger-analysis`.
        *   **Response:** `{ "jobId": "...", "status": ("pending" | "running" | "completed" | "failed"), "progress": "...", "resultUrl": "..." (if completed, and if results aren't directly updating the main data store queried by GET display endpoints) }`.
        *   **Services Used:** `DatabaseService` (to query `AnalysisJob` status).
-   *(Note on other potential trigger endpoints: While older or separate systems might have individual, synchronous trigger endpoints like `POST /analyses/wallets/{walletAddress}/pnl`, these are **not** the primary mechanism for the dashboard. The dashboard should use the asynchronous Unified Trigger Pattern above to initiate analyses. The existing `GET` endpoints like `/pnl-overview`, `/behavior-analysis` are then used to display the results once the analysis job is complete).*

### 4.4 Service Layer Interaction
- API controllers/handlers (e.g., in `src/api/controllers/`) will:
    1. Perform request validation and parse parameters.
    2. Authenticate the request via API key, retrieve `userId`.
    3. Call `databaseService.logActivity()` at the start of the request, and then again for success/failure outcomes, capturing duration, request parameters, and `userId`.
    4. Instantiate and invoke the appropriate NestJS-wrapped service(s) from locations like `src/behavior/behavior.service.ts`, `src/database/database.service.ts`, etc.
    5. **Note on User Context in Services (Refinement of original Task 4 "Refactor Services for API User Context"):** 
        - Core underlying services (e.g., `OriginalBehaviorService` in `src/wallet_analysis/core/`) will generally remain agnostic to API `userId` and direct `logActivity` calls. This maintains their reusability for CLIs, workers, etc.
        - NestJS wrapper services (e.g., `src/behavior/behavior.service.ts`) that are directly called by API controllers will receive the `userId` if they orchestrate multiple distinct, significant sub-operations that require separate activity log entries beyond the main API call logging. For simple pass-through calls from the wrapper to an original service (like the current `BehaviorService.getWalletBehavior`), controller-level logging is sufficient.
        - The original services will not be modified to directly accept `userId` for logging purposes unless a compelling case arises for a specific service where internal steps are complex and directly relevant to user-initiated API actions.
    6. Format service responses for the API.

### 4.5 Pre-computation vs. On-the-fly API Calculation
- The API will primarily serve pre-calculated data stored by the analysis scripts/services (e.g., `AnalysisResult`, `AdvancedStatsResult`).
- Minor data transformations, filtering, or combining results from different service calls can occur in the API layer or services.
- Computationally intensive analysis will continue to be handled by the core analyzers and services, with results persisted to the database. The API can trigger these processes if `Analysis Triggering Endpoints` are implemented.

### 4.6 Relationship with Existing Scripts
- **Coexistence:** Existing scripts (e.g., `activityCorrelator.ts`, `helius-analyzer.ts`) can continue to operate for batch processing, scheduled tasks, or direct CLI use.
- **API for On-Demand:** The API layer primarily serves interactive, on-demand requests, typical of a dashboard interface.
- **Future Evolution:** Some script functionalities might eventually be triggerable via the API or refactored to use API endpoints for consistency, but this is not an immediate priority. The focus is on exposing read-heavy data and core analysis results via the API first.

## Phase 5: Dashboard Integration (Ongoing - Largerly complete)

**Goal:** Develop a web-based dashboard that consumes the API layer to provide users with insights into wallet analysis data, focusing on intuitive UX, high-signal data presentation, and rapid insight extraction.

**Guiding Principles (from `docs/dash_details_thoughts.md`):**
- **Insight-Driven Design:** Every element should serve insight extraction, not just data presentation. Structure, labels, and navigation must accelerate pattern recognition and minimize friction.
- **Data Prioritization (Cognitive Flow Hierarchy):**
    - Top Left (Identity): Wallet address, balance, linked socials, active status (anchoring trust and relevance).
    - Top Right (Snapshot Metrics): P&L delta, winrate, behavior tag (e.g., "high-frequency sniper", "passive holder").
- **Metric Design (Insight Density):** Focus on ratio metrics and change metrics (e.g., winrate-to-volume ratio, profit consistency). Use clear visual cues (colors, icons) for trends.
- **Time Filter:** A single, master time filter affecting all dashboard panels, with sensible defaults and presets.
- **Behavioral Insight Presentation:** Use descriptive labels (e.g., "How Often This Wallet Acts") and comparative tags (e.g., "Faster than 82% of wallets").
- **UX Flow Optimization:** Collapsible sidebar, ubiquitous tooltips, hover-to-reveal context, and click-to-drilldown functionality.
- **Modular & Composable Components:** Prioritize data clarity and minimalism. Avoid visual clutter.

### 5.1 Dashboard Technology Stack & Architecture

-   **Framework:** Next.js (App Router)
-   **Styling:** TailwindCSS
-   **Core Component Library:** shadcn/ui (for primitives like tabs, buttons, dropdowns, cards, tables, date pickers)
-   **Metric Display & Layout Blocks:** Tremor (for clean metrics display, cards, grids, flex components)
-   **Charting:** Apache ECharts (for advanced/custom behavioral and temporal charts, e.g., heatmaps, bar ranges, session clusters) and mini sparkline charts (Tremor-compatible or simple ECharts) within metric cards.
    *   **Recommendation:** Develop a reusable `ChartWrapper` component or abstraction layer early (e.g., during Week 3). This wrapper should standardize common props (e.g., data, themes, loading states, error handling) and encapsulate ECharts instance setup and updates to reduce boilerplate, improve maintainability, and ensure consistency across different charts. - **COMPLETE (`EChartComponent.tsx` created)**
-   **Data Fetching:** SWR or React Query (for robust API integration, caching, and state synchronization).
    *   **Consideration for Data Schema Evolution:** Implement robust data handling strategies, including:
        *   Type guards for API response validation.
        *   Error boundaries at component and view levels.
        *   Graceful degradation or fallback UIs if expected data fields are missing or malformed, especially if the backend API schema is still evolving.
-   **State Management:** Zustand or React Context (for global state like selected wallet, time filter, and shared UI state; prefer lightweight options)

### 5.2 Dashboard Structure and API Mapping (based on user diagram and `dash_details_thoughts.md`)

-   **Sidebar (Collapsible):**
    *   Navigation: Wallet selection/repository, Wallet List, Settings, Documentation, Help/About.
    *   Implementation: shadcn/ui components. - **COMPLETE. Enhancements: Tooltips added for collapsed state. "Settings" and "Help" moved to a visually distinct footer group.** (If sidebar is still "full of shit", a specific new issue needs to be raised under UX Polish).
-   **Main Window - Top Section (Sticky Summary Bar):**
    *   **Left Part: General Account Info:** Wallet address, balance, linked socials (if available).
        *   Data: Primarily from `GET /wallets/{walletAddress}/summary`.
    *   **Right Part: Account Summary Snapshot:** Last active date, P&L snapshot (e.g., 24h/7d change), key behavior profile/category tag.
        *   Data: Primarily from `GET /wallets/{walletAddress}/summary`.
    *   Implementation: Tremor `Card`, `Metric`, `Text`, `Flex`, `Grid` components. - **COMPLETE. Section is now collapsible. `AccountSummaryCard.tsx` significantly refined for compactness (prioritizing PNL, Win Rate, Balance), added tooltips for secondary info (Last Active, Behavior; "Days Active" removed), and implemented user-provided HTML structure for secondary info grid and tooltips.**
-   **Universal Time Filter:** - **LARGELY COMPLETE** (Functionality is there, minor UI polish or preset adjustments could be ongoing)
-   **Main Window - Bottom Section (Tabbed Data Views):**
    *   Container: `WalletTabsContainer` using shadcn/ui `Tabs` component. - **COMPLETE. Tab bar made slimmer with reduced padding, subtle background, icons next to labels, and tooltips on triggers.** (Active tab styling remains a separate visual issue if not resolved by other means).
    *   **Tab 1: Token Performance** - **COMPLETE**
    *   **Tab 2: Account Stats & PNL** - **COMPLETE** (Marked as LARGELY COMPLETE earlier, but based on functionality, it seems complete. Further polish is in the UX list).
    *   **Tab 3: Behavioral Patterns** - **COMPLETE**
    *   **Tab 4: Reviewer Log / Notes** - **COMPLETE**

#### 5.2.1 Consistent UI States: Guidelines for Loading, Empty, and Error Handling

This section outlines the design principles, visual guidelines, and specific scenarios for implementing consistent loading, empty, and error states across the dashboard. The goal is to provide a professional, informative, and predictable user experience. The primary component for many of these states is `dashboard/src/components/shared/EmptyState.tsx`.

**A. Design Principles:**

1.  **Clarity:** Users should immediately understand what is happening (e.g., "Loading data...", "No transactions found for this period.", "Could not fetch wallet summary.").
2.  **Consistency:** Similar situations should be handled with similar visual cues and messaging patterns across the application.
3.  **Guidance:** When possible, states should guide the user towards a resolution or next step (e.g., "Try adjusting your filters," "Retry analysis," "Check API Key configuration.").
4.  **Non-Disruptiveness:** Loading and error states should be presented in a way that minimizes user frustration. For partial content loading, consider inline indicators or skeleton loaders over full-screen overlays.
5.  **Professionalism:** States should maintain the overall look and feel of the application.

**B. Visual Guidelines & Component Usage:**

1.  **`EmptyState.tsx` Component:**
    *   **Variants:** Utilize the predefined variants (`default`, `error`, `info`, `playful`, `loading`) appropriately.
        *   `default` (often with `icon={Loader2}`): For general loading states of entire tabs or main content cards.
        *   `error`: For API errors or critical client-side errors.
        *   `info`: For informational messages, like "No data available," "Data not yet generated," or specific contextual information.
        *   `playful`: Can be used for empty states where a more engaging or less formal message is appropriate (e.g., a brand new user with no wallets - currently not a primary use case in data-heavy tabs).
    *   **Props:**
        *   `icon`: Use appropriate Lucide icons (e.g., `Loader2` for loading, `AlertTriangle` for error, `Info` or `SearchX` for info/no results).
        *   `title`: Clear, concise title for the state.
        *   `description`: Further explanation or guidance.
        *   `actionButton`: Include if a direct call to action is relevant (e.g., "Retry", "Analyze Wallet"). Associated props: `onActionClick`, `isActionLoading`.

2.  **Skeleton Loaders (shadcn/ui `Skeleton`):**
    *   While `EmptyState` with `Loader2` is the primary mechanism for loading entire tabs or main card components, `Skeleton` components *can* be used for more granular, component-level loading states where the structure of the content is known (e.g., simulating text lines or simple shapes within a component before its specific data arrives). This provides a UX enhancement by showing the shape of the content to come. Currently, this is not widely implemented for main content blocks but remains a good practice for smaller, internal parts.

3.  **Toasts (shadcn/ui `useToast`):**
    *   For non-modal notifications like success messages (e.g., "Wallet added to favorites," "Analysis started"), warnings, or less critical errors that don't require a full `EmptyState` display.
    *   Use `variant: 'destructive'` for error toasts.

4.  **Inline Indicators:**
    *   For very localized loading (e.g., a single metric updating, or search input loading indicator), a small spinner (`Loader2` icon with `animate-spin`) next to the element can be less intrusive. (As seen in `WalletSearch.tsx`).

5.  **Button States (shadcn/ui `Button`):**
    *   Utilize the `disabled` prop and embed a `Loader2` icon (with `animate-spin`) within buttons that trigger asynchronous actions to provide visual feedback during the action (e.g., "Import & Analyze" button in `WalletSearch.tsx`).

**C. Specific Scenarios & Recommended Messages/Actions:**

1.  **Loading States:**
    *   **Full Page/Tab/Main Card Loading:**
        *   **Visual:** `EmptyState` component with `variant="default"` and `icon={Loader2}`.
        *   **Title:** "Loading [Page/Tab Name]..." or "Loading Data..." (e.g., "Loading Token Performance...", "Loading PNL Data...").
        *   **Description:** (Optional but recommended) "Please wait while we fetch the latest data." or a more specific message.
    *   **Component-Level Loading (e.g., a specific chart or complex internal part of a card, if not using the main EmptyState):**
        *   **Visual:** `Skeleton` components matching the component's structure, or a compact `EmptyState` with `variant="default"` and `icon={Loader2}`, or an inline spinner.
        *   **Title (if `EmptyState` used):** "Loading..."
    *   **Background Data Refresh (SWR revalidation):**
        *   Generally, no explicit global loading state is needed if data is already displayed. User-initiated refreshes that take noticeable time provide feedback via button states (e.g., "Refresh Wallet Analysis" button in `WalletProfileLayout.tsx`).
    *   **Global Analysis In Progress (`isAnalyzingGlobal`):**
        *   **Visual:** `EmptyState` component with `variant="default"` and `icon={Loader2}`.
        *   **Title:** "Analyzing Wallet..."
        *   **Description:** "Please wait while the wallet analysis is in progress. [Specific data type, e.g., PNL data] will update shortly."

2.  **Empty States:**
    *   **No Data for Selected Criteria (e.g., filters, time range):**
        *   **Visual:** `EmptyState` with `info` variant.
        *   **Icon:** `SearchX` or `Info`.
        *   **Title:** "No Data Found" or "No [Items] Match Your Criteria" (e.g., "No Transactions Match Your Filters", "No Data Found for Applied Filters").
        *   **Description:** "Try adjusting your filters or expanding the time range."
        *   **Action:** (Optional) Button to "Clear Filters" or "Reset View."
    *   **New Wallet / Data Not Yet Generated (after analysis, but specific data is missing or initial state):**
        *   **Visual:** `EmptyState` with `info` variant.
        *   **Icon:** `Info`, `SearchX`, or a relevant icon like `Users` (for behavior).
        *   **Title:** "[Data Type] Not Yet Available/Generated" (e.g., "PNL Data Not Yet Available", "Behavioral Profile Not Generated", "PNL Data Not Generated").
        *   **Description:** Provides context, e.g., "Comprehensive PNL data is not available for this wallet yet. It may need to be analyzed to generate these insights." or "PNL data has not been generated for this wallet, or no activity falls within the selected period. Please analyze the wallet." or detailed explanations like in `AccountStatsPnlTab.tsx` based on `lastAnalysisTimestamp`.
        *   **Action:** `Button` "Analyze Wallet" or "Refresh Wallet Data" (triggers analysis API).
    *   **No Transactions Found (after analysis, for a specific period where applicable):**
        *   **Visual:** `EmptyState` with `info` variant.
        *   **Icon:** `Info` or `SearchX`.
        *   **Title:** "No Transactions Recorded" or similar.
        *   **Description:** "There were no transactions for this wallet within the selected period."
    *   **No Search Results (e.g., wallet search in sidebar):**
        *   **Component:** `WalletSearch.tsx` handles this with inline text and an "Import & Analyze" button.
        *   **Message Example:** "No wallets found matching "{query}"."
        *   **Action:** "Import & Analyze" button.
    *   **Empty Favorite Wallets List:**
        *   **Component:** `FavoriteWalletsList.tsx` handles this using a structure similar to `EmptyState`.
        *   **Message Example:** "No favorite wallets yet." (Icon: `Star`).
    *   **API Key Not Configured (for features requiring it):**
        *   **Visual:** `EmptyState` (or similar custom div structure as in `FavoriteWalletsList` and `WalletSearch`).
        *   **Icon:** `Info`.
        *   **Title:** "API Key Required" or similar.
        *   **Description:** "This feature requires API Key configuration. Please set NEXT_PUBLIC_API_KEY."

3.  **Error States:**
    *   **API Errors (Failed to fetch data for a tab/component):**
        *   **Visual:** `EmptyState` with `error` variant.
        *   **Icon:** `AlertTriangle`.
        *   **Title:** "Error Loading Data" or "Could Not Fetch [Data Type]" (e.g., "Error Loading Behavioral Data", "Error Fetching PNL Data").
        *   **Description:** User-friendly message like `error.message` or a default like "An unexpected error occurred. Please try again." (Optionally include status code if helpful and not too technical).
        *   **Action:** `Button` "Retry Analysis" or "Retry" (re-fetches data or triggers analysis).
    *   **Specific 404 Errors (Data not found, distinct from general empty state):**
        *   Many components handle 404s by showing a specific `EmptyState` with an `info` variant, guiding the user to analyze the wallet (e.g., "PNL Data Not Yet Available" or "Behavioral Profile Not Generated"). This is preferable to a generic error for 404s where analysis is the solution.
    *   **Client-Side Processing Errors (Unexpected error in component):**
        *   **Visual:** `EmptyState` with `error` variant or a React Error Boundary for more critical failures.
        *   **Icon:** `AlertTriangle`.
        *   **Title:** "An Unexpected Error Occurred"
        *   **Description:** "Something went wrong while displaying this section. Please try refreshing the page."
    *   **Specific Action Failure (e.g., failing to add a favorite, trigger analysis from search):**
        *   **Visual:** Toast notification with `variant: 'destructive'`.
        *   **Title:** "Action Failed" (e.g., "Failed to Add Favorite", "Analysis error").
        *   **Description:** Brief reason from `error.message`.

**D. Testing Strategy Integration:**

*   During the "Comprehensive Review & Implementation Pass" (Task 3), each component will be tested against these scenarios:
    *   **Loading:** Simulate delayed API responses or use Storybook/testing utilities to force loading states. Verify correct display of skeletons or loading messages.
    *   **Empty:** Mock API responses with empty datasets or set up application state to reflect no data conditions. Verify appropriate empty state messages and CTAs.
    *   **Error:** Mock API error responses (e.g., 404, 500, network error) or introduce client-side errors. Verify correct error messages, icons, and retry mechanisms.
*   Check for console errors or warnings related to unhandled states.
*   Ensure consistency in terminology, iconography, and layout across all states.

### 5.3 Development Plan & Execution Steps (4-Week Iterative Cycle Example)

**Overarching Goals:**
-   API Layer First: Ensure necessary API endpoints from Phase 4 are stable and provide all data for a given view before starting its UI implementation.
-   Iterative Development: Build dashboard views one by one. Refine API based on frontend needs.
-   Component Reusability: Design modular components (e.g., `MetricCard`, `TimeRangeSelector`).

**Week 1: Project Setup & Layout Skeleton** - **COMPLETE**
*   **Tasks:**
    1.  Initialize Monorepo (if not already) or Frontend Project: Next.js with App Router. - COMPLETE
    2.  Integrate TailwindCSS. - COMPLETE
    3.  Configure shadcn/ui: Initialize, add necessary primitive components (buttons, tabs, dropdowns, cards, tables, date pickers). - COMPLETE
    4.  Integrate Tremor: Add to project for metric display components. - COMPLETE
    5.  Integrate Apache ECharts. - COMPLETE
    6.  Setup Data Fetching Library (SWR/React Query) and State Management (Zustand/Context). - COMPLETE
    7.  Define Routing: Main pages (`/wallets/[walletAddress]`, `/settings`, `/help`). - COMPLETE
    8.  Sidebar Implementation: Use shadcn/ui for structure and navigation items. - COMPLETE
    9.  Main Layout Skeleton (`WalletProfileLayout`):
        *   Top section: Placeholders for "General Account Info" (left) and "Account Summary Snapshot" (right). - COMPLETE
        *   Bottom section: Tab container (`WalletTabsContainer` with shadcn/ui `Tabs`) with stubs for the 4-5 data panels. - COMPLETE
        *   Use Tailwind grid/flex for responsive layout. - COMPLETE
    10. Component Stubs: `AccountSummaryCard`, `TimeRangeSelector`. - COMPLETE. **`AccountSummaryCard` significantly refined (see 5.2).**

**Week 2: Time Filter, Sticky Summary & "Token Performance" Tab** - **COMPLETE**
*   **Tasks:**
    1.  `TimeRangeSelector` Implementation:
        *   Build dropdown/date picker controls using shadcn/ui. - COMPLETE
        *   Store selected range globally using Zustand/Context. - COMPLETE
        *   Ensure API calls will use this global time range. - COMPLETE
    2.  Connect Sticky Summary Bar:
        *   Fetch data from `GET /wallets/{walletAddress}/summary` using SWR/React Query. - COMPLETE
        *   Populate "General Account Info" and "Account Summary Snapshot" using Tremor components. - **COMPLETE (Includes collapsible functionality and `AccountSummaryCard` refinements as noted in 5.2).**
    3.  "Token Performance" Tab Implementation: - COMPLETE
        *   Create data fetching hook `useTokenPerformance(walletId, timeRange, page, pageSize, sortBy, sortOrder)` using SWR/React Query to call `GET /wallets/{walletAddress}/token-performance`.
        *   Display data in a shadcn/ui `Table` with pagination and sorting. - COMPLETE
        *   Include columns like Token, P&L, Trade Count, Avg Hold Time. - COMPLETE
        *   **Detailed features implemented:** Quick filters ("Show Holdings Only", PNL range, text search, "Min. 2 Trades"), current UI balance display with formatting, ROI calculation, tooltips for token addresses, badges for token status (Held, Exited, Active). - COMPLETE
    4.  Integrate basic loading states and error handling for API calls. - COMPLETE

**Week 3: "Account Stats & PNL" and "Behavioral Patterns" Tabs** - **COMPLETE**
*   **Tasks:**
    1.  "Account Stats & PNL" Tab Implementation: - COMPLETE
        *   Fetch data from `GET /wallets/{walletAddress}/pnl-overview` and parts of `GET /wallets/{walletAddress}/summary`. - COMPLETE
        *   Use Tremor `Metric`, `Card`, `Grid`, `Flex` to render key stats (Overall PNL, Winrate, Avg PNL/Day, Profit Consistency Index). - COMPLETE
        *   Incorporate ECharts for simple distributions if valuable (e.g., PNL distribution). - (Deferred)
    2.  "Behavioral Patterns" Tab Implementation: - COMPLETE
        *   Fetch data from `GET /wallets/{walletAddress}/behavior-analysis`. - COMPLETE
        *   Display trader classification, strategic tags using Tremor `Text`, `Badge`. - COMPLETE
        *   Implement ECharts for: - **COMPLETE**
            *   Activity Heatmap (e.g., trades by hour of day / day of week). - **COMPLETE**
            *   Session Length Histogram. - **COMPLETE**
            *   Pattern Timelines (if applicable from API data). - **COMPLETE**
            *   **Note:** Reusable `EChartComponent.tsx` created and used. `CustomChart` import error fixed. Heatmap contrast improved.
        *   **Internal `BehavioralPatternsTab.tsx` Refactor:** Charts moved into a `Tabs` component with active tab styling. - **COMPLETE**
    3.  Ensure all tabs correctly respond to global `TimeRangeSelector` changes, re-fetching data as needed. - COMPLETE

**Week 4: "Reviewer Log/Notes" Tab, Polish & Export** - **COMPLETE** (Except for optional Export and Raw Transactions tab, which were not primary goals)
*   **Tasks:**
    1.  "Reviewer Log/Notes" Tab Implementation:
        *   Implement `EditableNotes` component with a shadcn/ui `Textarea` or a simple rich text editor.
        *   Set up API endpoints (`GET`/`POST` `/wallets/{walletAddress}/notes`) and connect the component with debounced autosave.
    2.  (Optional) "Raw Transactions" Tab:
        *   If implementing, fetch data from a new `GET /wallets/{walletAddress}/transactions` endpoint.
        *   Display in a shadcn/ui `Table` with sorting, filtering, and pagination.
    3.  UX Polish: - **PARTIALLY COMPLETE / ONGOING**
        *   **Completed Specifics:**
            *   Tab Bar (`WalletProfileLayout.tsx`): Made slimmer, reduced padding, subtle background, icons added, tooltips on triggers.
            *   Sidebar (`Sidebar.tsx`): Tooltips added for collapsed state, Settings/Help moved to footer.
            *   (Other `AccountSummaryCard` and Top Bar refinements detailed in Week 1 & 2 / Section 5.2)
        *   Add comprehensive tooltips (shadcn/ui `Tooltip`) for metrics and chart elements. - ONGOING
    4.  Export Options:
        *   Implement CSV export for "Token Performance" and "Raw Transactions" (if built) using `PapaParse`.
        *   (Optional) Implement PDF summary export for the "Sticky Summary Bar" or a condensed report using `jsPDF`.
    5.  Final review of component modularity and code clarity.

### 5.4 Future Enhancements (Post-Initial Build) (YET TO BE REFINE)
-   **Wallet Comparison View:** Ability to select two wallets and see their key metrics/tabs side-by-side.
-   **Leaderboards/Segment Comparison:** Compare selected wallet against predefined segments (e.g., top 10% PNL).
-   **Advanced Chart Interactions:** Drill-downs within charts, cross-filtering.
-   **User-Savable Views/Filters.**
-   **Integration with AI Assistant (Phase 6):** Expose dashboard data/queries to the LLM.
-   **Favorite Wallets Scalability:** Plan for and implement a more scalable solution for displaying a large number of favorite wallets. Options to consider include:
    *   A "View All Favorites" link in the sidebar that navigates to a dedicated page for managing and viewing all favorites, especially if the number exceeds a certain threshold (e.g., 20 items).
    *   Implementing virtual scrolling or pagination within the sidebar component itself if a dedicated page is not desired for the initial enhancement.
    *   Adding search/filter functionality within the favorites list.

### 5.5 Relationship with Existing Scripts & API
- This dashboard exclusively consumes the API layer defined in Phase 4.
- No direct database access from the frontend.
- All computationally intensive analysis is performed by backend services, with results served via API.
- The dashboard **must** be able to trigger re-analysis for a wallet. This will use the unified analysis triggering endpoints (defined in 4.3.5: `POST /analyses/wallets/{walletAddress}/trigger-analysis` and `GET /analyses/jobs/{jobId}/status`). The UI must provide clear feedback to the user about the status of these background jobs. (PAY EXTRA ATTENTION TO THIS - WE NEED TO BE ABLE TO EXECUTE ANALYSIS, FETCHING AND RE-ANALYSIS) - **This is a FUTURE item, as analysis triggering endpoints are marked (Future).** - **COMPLETE**

### 5.5.1 Auto-Loading Dashboard Wallet Analyses — Execution Plan *(Implemented)*

**Objective:** Deliver a progressive wallet experience that paints instantly, refreshes itself intelligently, and streams deeper history in layers without manual clicks or blocking the UI.

#### A. Experience Goals (✔ Implemented)
- **Instant context:** Cached snapshot renders immediately with freshness badge.
- **Progressive disclosure:** `flash → working → deep` scopes run sequentially; UI shows live status chips as richer data arrives.
- **User agency:** Manual CTA now routes through the scoped pipeline; contextual toasts differentiate auto vs manual triggers.
- **Predictable performance:** Flash scope constrained to ≤250 signatures (~7 days) for sub-5 s response; deeper scopes continue in background.

#### B. What Shipped
- `DashboardAnalysisRequest/ResponseDto` carry scope, history window, signature targets, trigger source, follow-up flags.
- Controller enforces per-scope freshness (flash 30 min, working 3 h, deep 12 h) and returns `skipped` metadata when short-circuiting.
- Processor adjusts sync options per scope, tracks signatures considered, records `AnalysisRun` markers, and queues follow-ups only when needed.
- Dashboard auto-triggers flash (session dedupe, demo guard), subscribes to websocket updates, merges results, and displays per-scope progress badges.
- Follow-up job IDs returned over websocket (`followUpJobsQueued`) to cascade subscriptions client-side.

#### C. Outstanding Edges
- **CTA copy / instrumentation:** Finalize copy (“Run full rebuild”, etc.) and log user actions once semantics finalize.
- **Testing:** After polish, run manual verification across high-volume wallets, low-activity wallets, demo users, multi-tab sessions, and follow-up-skip paths.

#### D. Next Checks
- Verify `DashboardAnalysisRequestDto.queueWorkingAfter/queueDeepAfter` truly align with UI defaults (flash auto queues working; working auto queues deep when stale).
- Add frontend guard to skip auto-trigger for restricted wallets and reconcile queued scopes without job IDs.
- Re-run `npm run verify` after installing dependencies to ensure type safety.

### 5.6 New Feature: Analysis Lab Page (Similarity Analysis UI) (COMPLETE)

This section details the plan for building the dedicated user interface to consume the `POST /api/v1/analyses/similarity` endpoint, bridging the gap between the raw backend service and a valuable user-facing feature.

#### 5.6.1 Goal & Location

*   **Goal:** To provide a dedicated, intuitive UI for users to run on-demand similarity analysis across a custom group of wallets and view the results in a structured, insight-driven dashboard.
*   **Location:** A new top-level page will be created at `/analysis-lab`, accessible from the main sidebar navigation. This dedicated space separates on-demand, multi-wallet analysis from single-wallet profile views and provides a home for future analysis tools.

#### 5.6.2 UI Design: Inputs & Configuration

The page will feature a flexible two-part input section to accommodate different user workflows.

1.  **Wallet Staging Area:**
    *   **Manual Input:** A primary search box will allow users to find wallets by address and add them to a "Staging List".
    *   **Bulk Input:** An "Import List" button will open a modal with a text area, allowing users to paste a list of addresses (e.g., comma-separated or one per line). A file upload option for `.csv` or `.txt` files will also be included.
    *   **Staging List:** A clear list of wallets queued for analysis will be displayed, with the ability to remove individual wallets before running the analysis.

2.  **Analysis Configuration:**
    *   A simple set of controls below the staging area will include:
        *   **Vector Type Selection:** A segmented control or radio button group for choosing between **`Capital`** (default) and **`Binary`** analysis types, with a tooltip explaining the distinction.
        *   **Run Button:** A "Run Analysis" button, disabled until at least two wallets are staged, which initiates the API call.

#### 5.6.3 UI Design: Output Display Strategy

The core of this feature is transforming the detailed report/JSON from the API into a digestible and interactive experience. Instead of a single text block, the results will be presented in four distinct, hierarchical components:

1.  **Key Insights & Summary Card:**
    *   **Purpose:** To provide immediate, high-level takeaways.
    *   **Content:** This card will prominently display the most critical findings from the report, such as "Significant Asymmetry," "Focused Investment Pattern," and "Very High Similarity / Strong Concordance" pairs.

2.  **Top 10 Most Similar Pairs Table:**
    *   **Purpose:** To highlight the most actionable results.
    *   **Content:** A clean table showing the top 10 pairs by similarity score. Each row will be expandable to reveal deeper context on demand, such as the top 5 shared tokens and their respective capital allocation percentages for each wallet in the pair.

3.  **All Pairs Connection Strength Table:**
    *   **Purpose:** To enable deep, exploratory analysis for power users.
    *   **Content:** A comprehensive data table of all wallet pairs. It will be sortable and filterable by metrics like Primary Score, Jaccard Score, and Shared Token Count. It will also display the qualitative "Insight Tags" (`*SC*`, `*FIP*`, etc.) for quick pattern recognition.

4.  **Most Common Tokens Card:**
    *   **Purpose:** To offer a quick overview of asset overlap across the entire set.
    *   **Content:** A small, secondary card listing the tokens that are most frequently shared among the analyzed wallets, along with the count of wallets they appear in.

This structured layout guides the user from the most important, pre-digested insights down to the most granular data, allowing for both quick glances and deep investigation.

## Phase 6: AI Assistant Integration

**Goal:** Integrate a powerful, flexible Large Language Model (LLM) service into the application to transform raw analytical data into high-value, human-readable insights. This will be achieved through two distinct, primary use cases.

**Core Component: The `LLMService`**
A central, reusable `LLMService` will be developed. This service will be designed to be modular and capable of:
- Accepting different types of input data (e.g., a JSON object with wallet summary stats, or a long string containing a full technical analysis report).
- Applying different, purpose-built prompts tailored to the specific analysis task.
- Interacting with a chosen LLM provider's API.
- Returning structured, clean output for use in the API and frontend.

---

### Use Case 1: The Wallet Profile Synthesizer (Lower Priority)

*   **Concept:** For any given wallet viewed on the dashboard, provide a concise, AI-generated narrative summary of its key characteristics.
*   **Input:** The JSON data already being fetched for the dashboard's summary, PNL, and behavior tabs.
*   **Process:** The `LLMService` will be called with this data and a specific prompt designed to synthesize it.
*   **Example Output:** "This wallet is a high-frequency trader with a 65% win rate, primarily focused on newly launched meme tokens. While its overall PNL is positive, it's highly volatile and driven by a few large wins. Recent activity shows a decrease in trading frequency."
*   **Value:** Provides an "at-a-glance" interpretation of complex data for users, improving the speed of insight.

---

### Use Case 2: The Expert Analysis Interpreter (High Priority)

*   **Concept:** Allow users to trigger complex, computationally intensive analyses (like Similarity or Correlation) on-demand for a set of wallets, and then use an LLM to translate the dense, technical report into an actionable, strategic summary.
*   **Input:** The raw text-based report generated by existing services (e.g., `SimilarityService`), as seen in `similarity_capital_similarity_report.md`.
*   **Process:**
    1.  A user triggers the analysis from the frontend.
    2.  The backend runs the appropriate analysis service (e.g., `SimilarityService`).
    3.  The raw report output is passed to the `LLMService` with a highly specialized "forensic analyst" prompt (as detailed in `docs/behavioral_reconstruction_task.md`).
    4.  The LLM interprets the report and identifies clusters, mirrors, bots, and strategic divergences.
*   **Value:** This is the core value proposition. It makes the system's most powerful and unique analytical capabilities accessible and understandable to any user, transforming a wall of text into strategic intelligence.

## Phase 7: Performance & Asynchronous Operations (Future Enhancement)

**Trigger:** API endpoints for triggering new analyses (see 4.3.5) become slow or block client requests for too long; need to process multiple analysis requests concurrently without overwhelming system resources.

**WAYS TO BE USED**: 3 Use cases so far: 

- Escape racing conditions when doing lots of operations (like in the similarity lab - similarity endpoint is triggering too many things). Immediate use case! 
- Long running operations - 50k transactions processing done in parts or in background by workers
- Running token metadata enrichment as background process independend from the main work - LIFO might help, to start it in the background as soon as we fetch transactions and keeping market cap live + data for tokens;

**Smart Choices (aligned with `docs/scaling.md` Phase 2):

### 7.1 Job Queue System Introduction - DONE 
- **Technology:** Introduce a job queue system (e.g., BullMQ) with Redis as the broker.
- **Purpose:** Offload long-running analysis tasks (PNL, Behavior, Correlation, Similarity) initiated via API calls to background workers.

### 7.2 API Integration with Job Queue
- API endpoints that trigger analyses (e.g., `POST /analyses/wallets/{walletAddress}/pnl`) will now:
    1. Validate the request and parameters.
    2. Add a job to the appropriate queue (e.g., `pnl-analysis-queue`).
    3. Return an immediate response (e.g., `202 Accepted` with a job ID).
- New API endpoints might be needed to check the status of a job (`GET /analyses/jobs/{jobId}`).

### 7.3 Worker Services Implementation
- Develop separate Node.js worker processes for each type of analysis or a generic worker that can handle different job types.
- Workers will listen to their respective queues.
- Upon receiving a job, a worker will:
    1. Instantiate and use the relevant services (`PnlAnalysisService`, `BehaviorService`, etc.) to perform the analysis.
    2. Update the job status in the queue and potentially in the `AnalysisRun` table in the database.
    3. Handle errors gracefully and update job status accordingly.

### 7.4 Benefits
- **Non-blocking API:** Improves API responsiveness for analysis-triggering requests.
- **Scalability:** Worker processes can be scaled independently based on load.
- **Resilience:** Jobs can be retried if workers fail (if queue is configured for retries).

## Phase 8: Advanced Database Scalability & Optimization (Future Enhancement)

**Trigger:** SQLite performance degrades significantly with a very large volume of transactions across many wallets; write contention becomes an issue; complex analytical queries for the dashboard become too slow despite optimizations.

**Smart Choices (aligned with `docs/scaling.md` Phase 4):

### 8.1 Database Migration to PostgreSQL
- **Action:** Use Prisma's migration capabilities to transition the database from SQLite to PostgreSQL.
- **Benefits:** PostgreSQL offers superior concurrency, scalability, robustness, and advanced features suitable for larger datasets and higher transaction volumes.
- **Impact:** Application code using Prisma Client will remain largely unchanged, demonstrating the power of the ORM.

### 8.2 Advanced Caching Layer
- **Technology:** Expand the use of Redis (introduced with job queues) or a similar caching solution.
- **Strategy:** Implement caching for frequently accessed, computationally expensive, or relatively static data:
    - Wallet summaries or frequently requested aggregated metrics from the API.
    - Results of common queries to `DatabaseService`.
- **Invalidation:** Develop a clear cache invalidation strategy to ensure data consistency.

### 8.3 Analytical Acceleration (If Required for Complex Dashboarding)
- **Context:** If dashboard queries remain slow even with PostgreSQL and caching, for highly complex, read-heavy analytics.
- **Options:**
    - **DuckDB:** For local or embedded analytics, potentially querying Parquet exports or a read replica of PostgreSQL.
    - **ETL + Data Warehouse (for very large scale):** Implement an ETL process to move data from PostgreSQL to a specialized analytical store (e.g., ClickHouse, BigQuery) if the application evolves into a large multi-user service requiring extensive BI capabilities. This is a significant architectural step and considered far-future.

How to pivot from current situation: 

Phase 1: Database Migration (2-3 weeks)
Set up PostgreSQL alongside SQLite
Migrate critical tables first
Implement dual-write pattern
Gradually move read operations

Phase 2: Service Extraction (3-4 weeks)
Extract analysis services into separate processes
Implement event-driven communication
Add service discovery and load balancing
Migrate job queues to message queues

Phase 3: Frontend Simplification (2-3 weeks)
Implement server state management
Simplify component state
Add proper caching layer
Optimize real-time updates


## Implementation Order (Updated)

1.  Type Consolidation (Done)
2.  Core Analysis Extraction (Done)
3.  Utility Function Extraction (Done)
4.  Service Implementation (Refinement for user context may be needed) (Largely Done)
5.  Configuration Type Refactoring (Done)
6.  Script Transformation (Done)
    - `activityCorrelator.ts` (Done)
    - `wallet-behavior-analyzer.ts` (Done)
    - `kpi-comparison-report.ts` (Done)
    - `helius-analyzer.ts` (Done)
    - `walletSimilarity.ts` (Done)
7.  **Phase 4: API Layer Implementation (COMPLETE)**
    *   7.1 Database schema changes for Users & ActivityLog. (Complete)
    *   7.2 `DatabaseService` enhancements for user/activity management. (Complete)
    *   7.3 API endpoint development (authentication, core read endpoints first, tech stack decision). (Complete).
    *   7.4 Service modifications to accept user context for logging. (Ongoing as needed / Review if actually needed now that core services are distinct from API wrappers)
8.  **Phase 5: Dashboard Integration (LARGELY COMPLETE - Focus shifting to UX Polish & Open Design Questions)**
    *   8.1 Dashboard skeleton and connection to summary API. (Complete)
    *   8.2 Iterative implementation of dashboard views:
        *   "Token Performance" tab. (Complete)
        *   "Account Stats & PNL" tab. (Complete)
        *   "Behavioral Patterns" tab. (Complete)
        *   "ReviewerLog / Notes" tab. (Complete)
    *   8.3 (Optional) Implementation of analysis triggering from dashboard. (Future - Depends on Phase 4.3.5 API endpoints)
9.  **Testing (Ongoing - See updated Testing Strategy)**
    *   9.1. Define test for the Mapper that will run periodically to detect changes in the logic of Helius!! 
10. **Phase 6: AI Assistant Integration (Experimental Pilot - NEW)**
    *   10.1 Define initial scope & use cases (e.g., natural language query for wallet summary).
    *   10.2 Design LLM tools based on existing API endpoints.
    *   10.3 Develop and test a pilot integration.
11. **Phase 7: Performance & Asynchronous Operations (Future Enhancement - formerly Phase 6)**
    *   11.1 Implement Job Queue System (e.g., BullMQ + Redis).
    *   11.2 Refactor API analysis triggers to use the job queue.
    *   11.3 Develop Worker Services.
12. **Phase 8: Advanced Database Scalability & Optimization (Future Enhancement - formerly Phase 7)**
    *   12.1 Migrate to PostgreSQL if SQLite limits are hit.
    *   12.2 Implement/Expand Caching Layer (Redis).
    *   12.3 Consider analytical acceleration tools if dashboarding complexity demands it.
13. (Further Future API Layer for Admin/Advanced Features)

## Phase 9: Identity & Access Management (User Registration + JWT, API Key Backward Compatibility)

### Goal
- Pivot from manual API key issuance to self-serve user registration and login while keeping existing API keys operational. Introduce JWT-based auth and a composite guard so both JWT and `X-API-Key` continue to work during transition.

### Minimal-Disruption Plan
1) Keep API key auth as-is; add JWT login alongside it.
2) Introduce a composite auth guard that accepts either Authorization: Bearer <jwt> or `X-API-Key`. Prefer JWT if present; fallback to API key.
3) Add basic auth endpoints for registration and login; support cookie or bearer modes via env toggles.

### Database Schema (Prisma)
- Extend `User`:
  - `email String? @unique`
  - `passwordHash String?`
  - `emailVerified Boolean @default(false)`
  - `lastLoginAt DateTime?`
- Keep existing `apiKey` (for programmatic access / compatibility).

### Backend Components (NestJS)
- Auth module:
  - `JwtModule` configured with `JWT_SECRET`, `JWT_EXPIRES_IN`.
  - `AuthService` (register/login with bcrypt).
  - `JwtStrategy` to validate JWTs.
- Composite guard (new):
  - If Authorization Bearer present → verify JWT → `request.user`.
  - Else if `X-API-Key` present → reuse API key guard/validation.
  - Else → 401.
  - Replace controller guards with this composite guard (single-line change per controller).

### Endpoints
- `POST /auth/register`: `{ email, password }` → create user, return JWT (and/or httpOnly cookie).
- `POST /auth/login`: `{ email, password }` → return JWT (and/or cookie).
- `GET /auth/me`: return current user.
- `POST /auth/logout`: clear cookie (if cookie mode enabled).

### Frontend (Dashboard) Minimal Changes
- Add sign-in/sign-up views.
- Update `fetcher` to include either httpOnly cookie (credentials: 'include') or Authorization bearer header.
- Gradually retire the “enter API key” UX; keep a section for programmatic key management.

### Environment Variables
- `JWT_SECRET`
- `JWT_EXPIRES_IN=7d`
- If cookie-based: `AUTH_COOKIE_NAME=analyzer.sid`, `AUTH_COOKIE_SECURE=true|false`, `FRONTEND_URL`, `BACKEND_URL`.
- Optional: `ALLOWED_WS_ORIGINS` for WebSocket CORS tightening.

### Rollout & Migration
- Phase A (compat): deploy composite guard; keep API keys working; enable register/login for selected users.
- Phase B (soft migrate): dashboard prefers JWT login; keep API keys for programmatic/CLI; add key rotate/revoke UI.
- Phase C (optional): restrict heavy endpoints to JWT users unless API key is flagged trusted.

### Security & Ops
- Bcrypt for password hashing; rate limit login; lockout/backoff on repeated failures.
- CORS: allow dashboard origin; cookie attributes if cookie mode.
- Logging: never log credentials; minimal logs.

## Phase 10: Real-time Ingestion via Helius Webhooks (with WS-assisted UX)

### Goal
- Transition from manual/periodic analysis to event-driven ingestion for tracked wallets using Helius Webhooks, while using WebSockets for low-latency UX on wallets users are actively viewing. Use Enhanced webhooks where possible, fall back to Raw for edge cases. References: [Webhooks Overview](https://www.helius.dev/docs/webhooks), [Enhanced](https://www.helius.dev/docs/webhooks#enhanced), [Raw](https://www.helius.dev/docs/webhooks#raw), [Transaction Types](https://www.helius.dev/docs/webhooks/transaction-types).

### Approach (Hybrid)
- Webhooks (primary ingestion)
  - Register “tracked wallets” (favorites, paying users, recently active) in a Helius webhook (Enhanced mode preferred).
  - Backend endpoint consumes webhook POSTs, verifies authenticity, dedupes/batches, and enqueues a small ingest job per wallet.
  - Ingest path does incremental DB updates and schedules analysis per policy (on-view, periodic, or threshold-based).
- WebSockets (supporting UX)
  - For wallets currently open in the dashboard, subscribe via WS to show instant “activity detected” and optimistic UI updates; the webhook pipeline remains the main data source.

### Enhanced vs Raw
- Prefer Enhanced to leverage decoded `type` (e.g., `SWAP`, `TRANSFER`, `BUY`, `SELL`) for selective analysis triggers. Use Raw only when Enhanced lacks required detail for specific programs. See [Transaction Types](https://www.helius.dev/docs/webhooks/transaction-types).

### Safeguard: High-traffic Wallet Demotion
- If a tracked wallet exceeds `X` tx/day (configurable), demote it from webhook ingestion to WS-triggered periodic sync:
  - Deregister from webhook; keep a lightweight WS tick subscription or poll to trigger batched `sync-wallet` every N minutes.
  - Re-promote after a cooling-off window if traffic normalizes. This limits costs/bot noise while preserving freshness.

### Components
1) API Endpoint
   - `HeliusWebhookController` (POST `/integrations/helius/webhook`)
     - Verify signature/secret from Helius settings.
     - Parse Enhanced/Raw payload; extract wallet addresses, signatures, and types.
     - Redis debounce per wallet (2–5s) to coalesce bursts; enqueue `ingest-wallet-events`.
2) Service & Job
   - `HeliusWebhookService`: verification, dedupe, enqueue policies.
   - `IngestWalletEventsProcessor`:
     - Idempotent upsert of new tx/signatures into our tables.
     - Policy scheduling: if wallet is open → enqueue analyze after ingest; else periodic or threshold-based.
     - Publish `wallet-updated` via existing `JobProgressGateway` for dashboard revalidation.
3) Subscription Management
   - On favorite add/remove → (un)register wallet with Helius webhook (persist mapping).
   - On startup → reconcile DB favorites vs Helius registrations.
4) Demotion Logic
   - Counters per wallet per day; when `> WEBHOOK_DEMOTION_TX_THRESHOLD`, mark auto-demoted and unregister from webhook; run WS-triggered periodic sync (`WS_PERIODIC_SYNC_MINUTES`).
   - Re-enable webhook when below threshold over a rolling window.
5) WebSocket (limited scope)
   - Subscribe only for wallets currently viewed; implement heartbeat/backoff/resubscribe as per [Standard WebSockets](https://www.helius.dev/docs/rpc/websocket).

### Data Flow
- Helius → POST webhook → controller → Redis debounce → enqueue ingest job → DB upsert → optional analyze → broadcast `wallet-updated` → dashboard revalidate.
- Demoted wallets: WS tick → Redis debounce (longer) → enqueue `sync-wallet` (deduped by existing lock/job id) → optional batched analyze.

### Idempotency & Load Control
- Redis keys:
  - `webhook:debounce:<wallet>` TTL 2–5s
  - `webhook:seen:<signature>` TTL 1–3 days
  - `webhook:count:<wallet>:<yyyy-mm-dd>` for demotion counters
- Per-wallet Redis lock (existing) prevents concurrent sync/analysis.
- Queue concurrency caps and DLQ retries enabled.

### Configuration (env)
- `HELIUS_WEBHOOK_SECRET`
- `HELIUS_WEBHOOK_MODE=enhanced|raw` (default enhanced)
- `WEBHOOK_DEBOUNCE_MS=5000`
- `WEBHOOK_DEMOTION_TX_THRESHOLD=2000`
- `WS_PERIODIC_SYNC_MINUTES=10`

### Minimal DB Additions (optional)
- `UserFavoriteWallet.autoTrack boolean default true`
- `Wallet.autoTrack boolean default true`
- `Wallet.isAutoDemoted boolean default false`

### Rollout
- Phase A: Implement endpoint + ingest, wire a few test wallets; validate idempotency and analysis triggers.
- Phase B: Enable for internal favorites, monitor counters and demotion behavior.
- Phase C: Roll out to all favorites/paying users; WS limited to active-view wallets.

## Testing Strategy (Updated)

# ======== Deployment Plan Archive ==========

This section contains the detailed, step-by-step plans for completed major operational tasks, retained for documentation and future reference.

### Initial Self-Hosted Deployment Details

*   **A. Backend Deployment (Self-Hosted on Raspberry Pi 5):**
    *   **1. Preparation & Code Deployment:**
        *   Ensure your NestJS backend project has a production build script (e.g., `npm run build` which typically outputs to a `dist` folder).
        *   Transfer the entire backend project (or at least the `dist` folder, `node_modules`, `package.json`, `.env` file) to your Raspberry Pi 5. (Using `git clone` and then `npm install --production` followed by `npm run build` on the RPi5 is often cleaner than transferring `node_modules`).
    *   **2. Environment Configuration (`.env` file on RPi5):**
        *   `DATABASE_URL`: Ensure this points to your SQLite file path (or PostgreSQL connection string if you migrate later) *as accessible from the RPi5 filesystem*.
        *   `PORT`: The port your NestJS application will listen on internally (e.g., `3001`).
        *   Any other API keys or secrets your backend application itself needs.
        *   `NODE_ENV=production`
    *   **3. PM2 Setup (Process Management):**
        *   Install PM2 globally if not already: `sudo npm install pm2 -g`.
        *   Create a PM2 Ecosystem File (e.g., `ecosystem.config.js`) in your backend project root on the RPi5. This provides a structured way to manage your app.
            *   **Example `ecosystem.config.js`:**
                ```javascript
                module.exports = {
                  apps : [{
                    name        : "my-analyzer-backend", // Choose a name
                    script      : "dist/main.js",     // Path to your NestJS entry point
                    watch       : false,              // Or true, or specify paths to watch
                    max_memory_restart : '1G',       // Optional: restart if it exceeds memory
                    env_production: {                // Environment variables for production
                       NODE_ENV: "production",
                       PORT: 3001, // Ensure this matches your app and .env
                       // DATABASE_URL can also be set here or preferably in .env loaded by your app
                    }
                  }]
                };
                ```
        *   Start your application using PM2: `pm2 start ecosystem.config.js --env production` (or just `pm2 start ecosystem.config.js` if `NODE_ENV` is in the file).
        *   Verify it's running: `pm2 list` or `pm2 logs my-analyzer-backend`.
        *   Save PM2 process list to resurrect on reboot: `pm2 save`.
        *   Enable PM2 startup script: `pm2 startup` (it will give you a command to run).
    *   **4. Public Accessibility (Router & DNS):**
        *   **Port Forwarding:** Configure your home router to forward an external port (e.g., `8080` or `3001` if your ISP doesn't block it; `443` if you set up HTTPS later) to your Raspberry Pi's internal IP address and the `PORT` your NestJS app is listening on (e.g., internal port `3001`).
        *   **Dynamic DNS (DDNS):** Set up a DDNS service (e.g., No-IP, DuckDNS) to get a stable hostname (e.g., `your-backend.duckdns.org`) that points to your home router's public IP address. Your router might have built-in DDNS client support, or you can run a DDNS client on the RPi5.
        *   **Firewall (on RPi5):** Ensure `ufw` (or other firewall) allows incoming connections on the port your NestJS app uses (e.g., `sudo ufw allow 3001/tcp`).
    *   **5. Nginx Reverse Proxy & Rate Limiting (Recommended for Robustness & HTTPS):**
        *   **Task:** Install and configure Nginx as a reverse proxy. This involves listening on standard ports (80/443), forwarding traffic to the NestJS app, obtaining and configuring SSL certificates (e.g., using Let's Encrypt), and implementing Layer 1 rate limiting.
        *   **Benefit:** Handles SSL encryption, can serve static files, provides an extra security layer via rate limiting, and can do basic load balancing.
        *   **API Rate Limiting (Two-Layer Strategy):**
            *   **Layer 1: Edge-Level Throttling (Infrastructure - Nginx)**
                *   **Status:** `To Do` (To be configured during this deployment task).
                *   **Action:** Configure Nginx to enforce a general, IP-based rate limit (e.g., 200 requests/minute/IP). This efficiently blocks high-volume, low-complexity attacks before they consume application resources.
            *   **Layer 2: Application-Level Throttling (Logic - NestJS)**
                *   **Status:** `Complete`.
                *   **Details:** The application has a robust, context-aware rate-limiting policy implemented using `nestjs-throttler`. A global limit of 100 requests/minute is in place, with a much stricter, user-aware limit of 3 requests/minute on the `trigger-analysis` endpoint to protect this expensive operation. The controller has also been improved to handle concurrent analyses for different wallets.

*   **B. Frontend Deployment (Vercel):**
    *   **1. Project Setup & Git:**
        *   Ensure your Next.js dashboard code (`dashboard` directory) is a Git repository and pushed to a provider like GitHub, GitLab, or Bitbucket.
    *   **2. Connecting to Vercel:**
        *   Sign up/log in to Vercel.
        *   Import your Git repository.
        *   Vercel typically auto-detects Next.js projects and configures build settings correctly. Review these if needed (e.g., root directory if your dashboard is in a subdirectory of the repo).
    *   **3. Environment Variables on Vercel:**
        *   In your Vercel project settings, navigate to "Environment Variables."
        *   Add `NEXT_PUBLIC_API_KEY`: The API key your frontend will use to authenticate with your backend. **This key is user-facing (public) in the sense that it's embedded in the frontend code, so it should be an API key that your backend uses to identify the *frontend application itself* or a *specific user*, not a master backend secret.**
        *   Add `NEXT_PUBLIC_API_BASE_URL`: The full public URL of your self-hosted RPi5 backend. This will be your DDNS hostname and the public port you forwarded (e.g., `http://your-backend.duckdns.org:8080/api/v1` or `https://your-backend.duckdns.org/api/v1` if you set up Nginx with HTTPS).
        *   **Modularity/Avoiding Lock-in:** Vercel environment variables are standard. The key is that your *application code* reads these standard environment variables (e.g., `process.env.NEXT_PUBLIC_API_BASE_URL`). If you move to another hosting provider, you'd just set up the same environment variables there. Your code doesn't need to change. Avoid using Vercel-specific APIs or features *for core functionality* if portability is a major concern.
    *   **4. Build & Deployment Settings (Review Defaults):**
        *   **Build Command:** Usually `npm run build` or `next build`.
        *   **Output Directory:** Usually `.next`.
        *   **Install Command:** Usually `npm install`.
        *   Vercel handles these well for Next.js.
    *   **5. Trigger Deployment:** Typically happens automatically on push to your main branch, or you can trigger manually.

*   **C. Post-Deployment Steps & Best Practices:**
    *   **1. Thorough Testing:** Access your dashboard via its Vercel URL. Test all functionalities, especially those involving API calls to your RPi5 backend (API key input, `trigger-analysis`, data fetching for all tabs).
        *   Pay attention to API call success/failure, loading times (especially `trigger-analysis` given RPi5 limitations).
    *   **2. Enhanced Landing Page & API Key Handling (User Experience & Access Control):**
        *   **Goal:** Provide a clear path for users to use their own API key for full access, while also offering a demo mode with pre-selected wallets for exploration without requiring a key for those specific demos.
        *   **Landing Page (`dashboard/src/app/page.tsx`):**
            *   Implement a prominent API key input field: "Enter API Key for Full Access".
            *   On submission, store the key in `localStorage`.
            *   Add a section: "Explore Demo Wallets" listing 2-3 predefined public demo wallet addresses. These links navigate to `/wallets/[demoWalletAddress]`.
        *   **Frontend API Logic (`fetcher.ts` & component calls):**
            *   When making API calls, `fetcher.ts` should retrieve the API key from `localStorage`. If present, include it in the `X-API-Key` header.
            *   If no key is in `localStorage` (e.g., user hasn't entered one), no `X-API-Key` header is sent for requests originating from navigating to a demo wallet profile or if the backend is set up to serve demo data without a key.
            *   For actions like using the main wallet search bar for a *non-demo* wallet or triggering analysis, if no API key is in `localStorage`, the UI should prompt the user to enter a key first or display an error message if a backend call fails due to a missing key.
        *   **Backend API Logic (NestJS - for Demo vs. Full Access Control - Option A from discussion):**
            *   Define a list of public demo wallet addresses (e.g., in `.env` or config).
            *   In an API authentication guard/middleware:
                *   If a request has a valid `X-API-Key` header: Allow access for any wallet.
                *   If no `X-API-Key` (or an invalid one) is present:
                    *   Check if the requested `walletAddress` (from route params) is in the predefined list of demo wallets.
                    *   If YES (it's a demo wallet): Allow the (read-only) request to proceed.
                    *   If NO (not a demo wallet): Return 401/403 error (e.g., "API Key required to analyze this wallet").
            *   The `trigger-analysis` endpoint (`POST /api/v1/analyses/wallets/:walletAddress/trigger-analysis`) MUST ALWAYS require a valid, non-demo API key. No new analyses for demo-mode users.
        *   **3. CORS (Cross-Origin Resource Sharing) on Backend:**
            *   Your NestJS backend on the RPi5 **must** be configured to allow requests from your Vercel frontend's domain. If not, browser security will block the API calls.
            *   In your NestJS `main.ts`, enable CORS: `app.enableCors({ origin: 'https://your-vercel-dashboard-url.vercel.app' });` (or use `*` for initial testing, but be more specific for production).
        *   **4. Logging & Monitoring (Basic):**
            *   **Backend (PM2):** Regularly check `pm2 logs my-analyzer-backend` on the RPi5 for errors or performance issues.
            *   **Frontend (Vercel):** Vercel provides deployment logs and some runtime logs/analytics.
        *   **5. Documentation (Internal):**
            *   Create a `DEPLOYMENT.md` file or a section in your main `README.md`.
            *   Document: DDNS setup, port forwarding rules, PM2 configuration, Vercel project settings (especially env var names), and the deployment process for both frontend and backend. This will be invaluable for your future self or collaborators.
        *   **6. Incremental Updates:** Once set up, deploying updates to the frontend on Vercel is usually as simple as pushing to your Git main branch. For the backend, you'd pull changes on the RPi5, rebuild, and restart the PM2 process (`pm2 restart my-analyzer-backend`).
        *   **7. Backup (RPi5):** Regularly back up your RPi5's SD card or at least your application code, database file, and critical configurations (`.env`, Nginx configs, PM2 ecosystem file).

*   **D. Considerations for Self-Hosting Backend (RPi5):**
    *   **Reliability:** Home internet connections and hardware can be less reliable than cloud providers. Be prepared for occasional downtime.
    *   **Security:** You are responsible for securing the RPi5 (OS updates, firewall, secure SSH, etc.).
    *   **Performance:** The RPi5 has limitations. The transaction insertion performance is a known point. Monitor resource usage (CPU, RAM) during heavy analysis.
    *   **ISP Terms of Service:** Some ISPs frown upon hosting public servers on residential connections. Usually not an issue for low-traffic personal projects.



**Known issues:**

- 01 P0 - MAPPER FAILING INTERACTION WITH DCA KEEPER - the mapper is failing when interacting with DCA KEEPR - make sure to fix it! Currently double-spending on ins

K3VYZgVA9snNCb17o6vVcEvR3gdHGVaa2PxNGbGqJXQsvrDRynZjMZ17tasAiyrGUzBNnVJdY1S35vnAPjierM8

